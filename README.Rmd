---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)

library(dplyr)
library(xxhashlite)
```

# xxhashlite

<!-- badges: start -->
![](https://img.shields.io/badge/cool-useless-green.svg)
<!-- badges: end -->

`xxhashlite` provides simple access to the *very* fast hashing functions
in [xxHash](https://cyan4973.github.io/xxHash/) for in-memory hashing 
of R atomic, numeric vectors.

The package provides support for hashing of vectors, matrices and arrays
which contain raw, integer, real or logical values.  To hash arbitrary R 
objects, use `base::serialize()` to first turn the object into 
a vector of raw bytes.

Currently xxHash code provided with this package is v0.7.3

### Limitations

* It is the underlying data of the vector or matrix that is being hashed, and this
 does not include any notion of the container for that data.  This
 means that a vector and array which contain the same data will hash to the 
 same value - regardless of the dimensions.
* `xxHash v0.7.x` includes the experimental `xxh3` and `xxh128` hash functionx.
   From the documentation: "The algorithm is currently in development, meaning 
   its return values might still change in future versions. However, the API is 
   stable, and can be used in production, typically for generation of 
   ephemeral hashes (produced and consumed in same session)".
* `xxHash` is a non-cryptographic hash.


## Installation

You can install from [GitHub](https://github.com/coolbutuseless/xxhashlite) with:

``` r
# install.package('remotes')
remotes::install_github('coolbutuseless/xxhashlite)
```

#### Optimisation via `Makevars`

To get the most out of what `xxHash` offers, it's important to set
some optimisation flags for your machine.  The important compiler flags to set 
are `-O3` and `-march=native`.

Here are 2 possible ways to do this:

1. Copy `src/Makevars.custom` to `src/Makevars` re-build package.
2. Edit your `~/.R/Makevars` to include `CFLAGS = -O3 -march=native` (this will
  change flags for all future compilation, and should probably be used with caution)


## Simple hashing

Four hash functions from xxHash's "simple api" are exposed:

* **xxhash32** - 32 bit output. Will be slow on a 64bit machine.
* **xxhash64** - 64 bit output. About 2x faster than xxhash32
* **xxhash128** - 128 bit output. Still marked as experimental in xxHash
* **xxh3** - 64 bit output. Still marked as experimental in xxHash


```{r example}
library(xxhashlite)

vec <- raw(1e6)

xxhashlite::xxhash32(vec)

xxhashlite::xxhash64(vec)

xxhashlite::xxhash128(vec)

xxhashlite::xxh3_64bits(vec)
```


## Hashing 1 million raw bytes

`xxhashlite::xxh3_64bits()` hashes data at around 25 GB/s for this input data size.

Since `digest::digest()` can hash raw bytes like `xxhashlite` the speeds for `xxhashlite::xxhash64()`
and `digest(algo = 'xxhash64')` are about the same.

There are some cryptographic hashes (from `blake3` and `openssl`) included in this 
benchmark.  These are *cryptographic* hashes, so this isn't really a fair 
comparison, but for those familiar with those hashes it's a good yardstick by 
which to measure the performace of `xxHash`

<details>
<summary> Click here to show/hide benchmark code </summary>

```{r bench1}
library(digest)
library(blake3)
library(openssl)
library(fastdigest)

N   <- 1e6
vec <- raw(N)

res <- bench::mark(
  xxhash32(vec),
  xxhash64(vec),
  xxhash128(vec),
  xxh3_64bits(vec),
  blake3_hash_raw(vec),
  sha1(vec),
  sha2(vec),
  sha224(vec),
  sha256(vec),
  sha384(vec),
  sha512(vec),
  digest(vec, algo = 'xxhash32', serialize = FALSE),
  digest(vec, algo = 'xxhash64', serialize = FALSE),
  fastdigest(vec),
  check = FALSE
)
```

</details>


```{r echo = FALSE}
res %>% 
  mutate(`GB/s` = round(N/1024^3 / as.numeric(median), 1)) %>%
  mutate(`itr/sec` = round(`itr/sec`)) %>%
  select(expression, median, `itr/sec`, `GB/s`) %>%
  mutate(package = c(
    rep('xxhashlite', 4),
    'blake3',
    rep('openssl', 6),
    rep('digest', 2),
    'fastdigest'
  )) %>%
  select(package, everything()) %>%
  knitr::kable(caption = "Hashing 1 million raw bytes")
```



## Hashing 1 thousand raw bytes

At smaller data sizes, the call overhead becomes more significant, and the 
overall throughput drops drastically.

`xxhashlite` is about 20x faster than `digest::digest()` in this case.

<details>
<summary> Click here to show/hide benchmark code </summary>

```{r}
N <- 1024L
vec <- raw(N)

res <- bench::mark(
  xxhash32(vec),
  xxhash64(vec),
  xxhash128(vec),
  xxh3_64bits(vec),
  digest(vec, algo = 'xxhash32', serialize = FALSE),
  digest(vec, algo = 'xxhash64', serialize = FALSE),
  fastdigest(vec),
  check = FALSE
)
```

</details>


```{r echo = FALSE}
res %>% 
  mutate(`GB/s` = round(N/1024^3 / as.numeric(median), 3)) %>%
  mutate(`itr/sec` = round(`itr/sec`)) %>%
  select(expression, median, `itr/sec`, `GB/s`) %>%
  mutate(package = c(
    rep('xxhashlite', 4),
    rep('digest', 2),
    'fastdigest'
  )) %>%
  select(package, everything()) %>%
  knitr::kable(caption = "Hashing 1000 raw bytes")
```


## Hashing 1 million numeric values

In the case of integers, doubles and logical values, `xxhashlite` will hash the
data values directly, while `digest::digest()` must first serialize them to 
raw bytes.

As a result, `xxhashlite::xxhash128()` is about 100x faster than `digest::digest()`, 
and operates at about 20 GB/s (on my machine).

<details>
<summary> Click here to show/hide benchmark code </summary>



```{r}
N   <- 1e6
vec <- numeric(N)

res <- bench::mark(
  xxhash32(vec),
  xxhash64(vec),
  xxhash128(vec),
  xxh3_64bits(vec),
  digest(vec, algo = 'xxhash32', serialize = TRUE),
  digest(vec, algo = 'xxhash64', serialize = TRUE),
  fastdigest(vec),
  check = FALSE
)
```

</details>

```{r echo = FALSE}
res %>% 
  mutate(`GB/s` = round(N*8/1024^3 / as.numeric(median), 1)) %>%
  mutate(`itr/sec` = round(`itr/sec`)) %>%
  select(expression, median, `itr/sec`, `GB/s`) %>%
  mutate(package = c(
    rep('xxhashlite', 4),
    rep('digest', 2),
    'fastdigest'
  )) %>%
  select(package, everything()) %>%
  knitr::kable(caption = "Hashing 1 million numeric values")
```


## Hashing 1 million integer values

In the case of integers, doubles and logical values, `xxhashlite` will hash the
data values directly, while `digest::digest()` must first serialize them to 
raw bytes.

The `hashFunction` package offers a few hashes that work directly with integer values.

`xxhashlite::xxhash128()` is about 100x faster than `digest::digest()`, and about
5x faster than `hashFunction` hashes.

<details>
<summary> Click here to show/hide benchmark code </summary>


```{r}
library(hashFunction)
N <- 1e6
vec <- integer(N)

res <- bench::mark(
  xxhash32(vec),
  xxhash64(vec),
  xxhash128(vec),
  xxh3_64bits(vec),
  digest(vec, algo = 'xxhash32', serialize = TRUE),
  digest(vec, algo = 'xxhash64', serialize = TRUE),
  fastdigest(vec),
  murmur3.32(vec),
  cityhash.64(vec),
  spooky.32(vec),
  check = FALSE
)
```

</details>

```{r echo = FALSE}
res %>% 
  mutate(`GB/s` = round(N*4/1024^3 / as.numeric(median), 1)) %>%
  mutate(`itr/sec` = round(`itr/sec`)) %>%
  select(expression, median, `itr/sec`, `GB/s`) %>%
  mutate(package = c(
    rep('xxhashlite', 4),
    rep('digest', 2),
    'fastdigest',
    rep('hashFunction', 3)
  )) %>%
  select(package, everything()) %>%
  knitr::kable(caption = "Hashing 1 million integer values")
```



## Related Software

* The original [xxHash](https://cyan4973.github.io/xxHash/) software.
* The [`digest`](https://cran.r-project.org/package=digest) package is 
  a much more comprehensive approach to hashing from within R - multiple 
  hashing functions and much more configurability
* The [`hashFunction`](https://cran.r-project.org/package=hashFunction) package offers 
  some hasing functions that work directly on character or integer values.
* The [`fastdigest`](https://cran.r-project.org/package=fastdigest) offers
  the fast non-cryptographic 'SpookyHash' and will hash anything that
  serialize knows about.

## Acknowledgements

* Yann Collett for releasing, maintaining and advancing [xxHash](https://cyan4973.github.io/xxHash/)
* R Core for developing and maintaining such a great language.
* CRAN maintainers, for patiently shepherding packages onto CRAN and maintaining
  the repository
